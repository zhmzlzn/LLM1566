#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»“æœåˆ†ææ¨¡å—
ç”¨äºåˆ†æç«æŠ€ç»“æœå¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import json
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Any
from collections import defaultdict
import seaborn as sns
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class ResultAnalyzer:
    """ç»“æœåˆ†æå™¨"""
    
    def __init__(self, result_data: Dict[str, Any]):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.result_data = result_data
        self.final_rankings = result_data.get('final_rankings', [])
        self.detailed_results = result_data.get('detailed_results', [])
        self.total_questions = result_data.get('total_questions', 0)
        self.total_rounds = result_data.get('total_rounds', 0)
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report = []
        report.append("# å¤§æ¨¡å‹ç«æŠ€ç»“æœåˆ†ææŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"\n## ç«æŠ€æ¦‚å†µ")
        report.append(f"- å‚èµ›æ¨¡å‹æ•°é‡ï¼š{len(self.final_rankings)}")
        report.append(f"- æ€»é—®é¢˜æ•°ï¼š{self.total_questions}")
        report.append(f"- æ€»è½®æ¬¡æ•°ï¼š{self.total_rounds}")
        
        # æœ€ç»ˆæ’å
        report.append(f"\n## æœ€ç»ˆæ’å")
        for i, (model_name, score) in enumerate(self.final_rankings):
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else "ğŸ…"
            report.append(f"{i+1}. {medal} **{model_name}**: {score} åˆ†")
        
        # æ€§èƒ½åˆ†æ
        performance_analysis = self._analyze_performance()
        report.append(f"\n## æ€§èƒ½åˆ†æ")
        report.append(f"- å¹³å‡å¾—åˆ†ï¼š{performance_analysis['average_score']:.2f}")
        report.append(f"- å¾—åˆ†æ ‡å‡†å·®ï¼š{performance_analysis['score_std']:.2f}")
        report.append(f"- æœ€é«˜å¾—åˆ†ï¼š{performance_analysis['max_score']}")
        report.append(f"- æœ€ä½å¾—åˆ†ï¼š{performance_analysis['min_score']}")
        
        # å„æ¨¡å‹è¡¨ç°åˆ†æ
        model_analysis = self._analyze_model_performance()
        report.append(f"\n## å„æ¨¡å‹è¯¦ç»†è¡¨ç°")
        for model_name, stats in model_analysis.items():
            report.append(f"\n### {model_name}")
            report.append(f"- æ€»å¾—åˆ†ï¼š{stats['total_score']}")
            report.append(f"- å¹³å‡å•é¢˜å¾—åˆ†ï¼š{stats['avg_score']:.2f}")
            report.append(f"- æœ€ä½³è¡¨ç°ï¼š{stats['best_performance']}")
            report.append(f"- æ“…é•¿é¢†åŸŸï¼š{', '.join(stats['strong_topics'])}")
            report.append(f"- ä½œä¸ºè£åˆ¤çš„å…¬æ­£æ€§è¯„åˆ†ï¼š{stats['judge_fairness']:.2f}")
        
        # é—®é¢˜éš¾åº¦åˆ†æ
        topic_analysis = self._analyze_by_topic()
        report.append(f"\n## é—®é¢˜ä¸»é¢˜åˆ†æ")
        for topic, stats in topic_analysis.items():
            report.append(f"\n### {topic}")
            report.append(f"- é—®é¢˜æ•°é‡ï¼š{stats['count']}")
            report.append(f"- å¹³å‡å¾—åˆ†ï¼š{stats['avg_score']:.2f}")
            report.append(f"- è¡¨ç°æœ€ä½³æ¨¡å‹ï¼š{stats['best_model']}")
        
        return "\n".join(report)
    
    def _analyze_performance(self) -> Dict[str, float]:
        """åˆ†ææ•´ä½“æ€§èƒ½"""
        scores = [score for _, score in self.final_rankings]
        return {
            'average_score': sum(scores) / len(scores) if scores else 0,
            'score_std': pd.Series(scores).std() if len(scores) > 1 else 0,
            'max_score': max(scores) if scores else 0,
            'min_score': min(scores) if scores else 0
        }
    
    def _analyze_model_performance(self) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„æ¨¡å‹è¡¨ç°"""
        model_stats = {}
        
        # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
        for model_name, total_score in self.final_rankings:
            model_stats[model_name] = {
                'total_score': total_score,
                'scores': [],
                'topics': defaultdict(list),
                'judge_scores': [],
                'best_performance': '',
                'strong_topics': []
            }
        
        # æ”¶é›†è¯¦ç»†æ•°æ®
        for result in self.detailed_results:
            judge = result['judge']
            rankings = result['rankings']
            question = result['question']
            
            # æå–é—®é¢˜ä¸»é¢˜ï¼ˆç®€å•æ–¹æ³•ï¼‰
            topic = self._extract_topic(question)
            
            for model_name, score in rankings:
                if model_name in model_stats:
                    model_stats[model_name]['scores'].append(score)
                    model_stats[model_name]['topics'][topic].append(score)
            
            # åˆ†æè£åˆ¤å…¬æ­£æ€§ï¼ˆåŸºäºå¾—åˆ†åˆ†å¸ƒçš„æ–¹å·®ï¼‰
            if judge in model_stats:
                scores = [score for _, score in rankings]
                score_variance = pd.Series(scores).var() if len(scores) > 1 else 0
                model_stats[judge]['judge_scores'].append(score_variance)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        for model_name, stats in model_stats.items():
            scores = stats['scores']
            stats['avg_score'] = sum(scores) / len(scores) if scores else 0
            
            # æ‰¾å‡ºæœ€ä½³è¡¨ç°
            if scores:
                max_score_idx = scores.index(max(scores))
                stats['best_performance'] = f"ç¬¬{max_score_idx + 1}é¢˜å¾—åˆ†{max(scores)}"
            
            # æ‰¾å‡ºæ“…é•¿é¢†åŸŸ
            topic_avgs = {}
            for topic, topic_scores in stats['topics'].items():
                if topic_scores:
                    topic_avgs[topic] = sum(topic_scores) / len(topic_scores)
            
            if topic_avgs:
                sorted_topics = sorted(topic_avgs.items(), key=lambda x: x[1], reverse=True)
                stats['strong_topics'] = [topic for topic, _ in sorted_topics[:2]]
            
            # è®¡ç®—è£åˆ¤å…¬æ­£æ€§ï¼ˆæ–¹å·®è¶Šå°è¶Šå…¬æ­£ï¼‰
            judge_scores = stats['judge_scores']
            stats['judge_fairness'] = 10 - (sum(judge_scores) / len(judge_scores)) if judge_scores else 5
            stats['judge_fairness'] = max(0, min(10, stats['judge_fairness']))  # é™åˆ¶åœ¨0-10èŒƒå›´
        
        return model_stats
    
    def _analyze_by_topic(self) -> Dict[str, Dict[str, Any]]:
        """æŒ‰ä¸»é¢˜åˆ†æ"""
        topic_stats = defaultdict(lambda: {
            'count': 0,
            'scores': [],
            'model_scores': defaultdict(list)
        })
        
        for result in self.detailed_results:
            question = result['question']
            rankings = result['rankings']
            topic = self._extract_topic(question)
            
            topic_stats[topic]['count'] += 1
            
            for model_name, score in rankings:
                topic_stats[topic]['scores'].append(score)
                topic_stats[topic]['model_scores'][model_name].append(score)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        final_topic_stats = {}
        for topic, stats in topic_stats.items():
            scores = stats['scores']
            model_scores = stats['model_scores']
            
            # æ‰¾å‡ºè¡¨ç°æœ€ä½³çš„æ¨¡å‹
            best_model = ''
            best_avg = 0
            for model_name, model_topic_scores in model_scores.items():
                if model_topic_scores:
                    avg_score = sum(model_topic_scores) / len(model_topic_scores)
                    if avg_score > best_avg:
                        best_avg = avg_score
                        best_model = model_name
            
            final_topic_stats[topic] = {
                'count': stats['count'],
                'avg_score': sum(scores) / len(scores) if scores else 0,
                'best_model': best_model
            }
        
        return final_topic_stats
    
    def _extract_topic(self, question: str) -> str:
        """ä»é—®é¢˜ä¸­æå–ä¸»é¢˜ï¼ˆç®€å•çš„å…³é”®è¯åŒ¹é…ï¼‰"""
        topic_keywords = {
            'é€»è¾‘æ¨ç†': ['æ¨ç†', 'é€»è¾‘', 'æ¨æ–­', 'åˆ¤æ–­'],
            'æ•°å­¦è®¡ç®—': ['è®¡ç®—', 'æ•°å­¦', 'æ–¹ç¨‹', 'å‡ ä½•', 'è¯æ˜'],
            'åˆ›æ„å†™ä½œ': ['å†™ä½œ', 'åˆ›ä½œ', 'æ–‡ç« ', 'æ•…äº‹', 'è¯—æ­Œ'],
            'çŸ¥è¯†é—®ç­”': ['è§£é‡Š', 'åˆ†æ', 'ä»€ä¹ˆæ˜¯', 'æ¯”è¾ƒ'],
            'ç¼–ç¨‹ç®—æ³•': ['ç®—æ³•', 'ç¼–ç¨‹', 'ä»£ç ', 'å®ç°', 'æ•°æ®ç»“æ„'],
            'ç³»ç»Ÿè®¾è®¡': ['è®¾è®¡', 'ç³»ç»Ÿ', 'æ¶æ„', 'åˆ†å¸ƒå¼'],
            'å•†ä¸šåˆ†æ': ['å•†ä¸š', 'ç­–ç•¥', 'åˆ†æ', 'å¸‚åœº', 'CEO']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in question for keyword in keywords):
                return topic
        
        return 'å…¶ä»–'
    
    def generate_charts(self, output_dir: str = "./charts"):
        """ç”Ÿæˆå›¾è¡¨"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. æœ€ç»ˆæ’åæŸ±çŠ¶å›¾
        self._plot_final_rankings(output_dir)
        
        # 2. å„æ¨¡å‹å¾—åˆ†åˆ†å¸ƒ
        self._plot_score_distribution(output_dir)
        
        # 3. ä¸»é¢˜è¡¨ç°çƒ­åŠ›å›¾
        self._plot_topic_heatmap(output_dir)
        
        # 4. è£åˆ¤å…¬æ­£æ€§åˆ†æ
        self._plot_judge_fairness(output_dir)
    
    def _plot_final_rankings(self, output_dir: str):
        """ç»˜åˆ¶æœ€ç»ˆæ’åå›¾"""
        models = [name for name, _ in self.final_rankings]
        scores = [score for _, score in self.final_rankings]
        
        plt.figure(figsize=(12, 6))
        bars = plt.bar(models, scores, color=['gold', 'silver', '#CD7F32'] + ['skyblue'] * (len(models) - 3))
        plt.title('å¤§æ¨¡å‹ç«æŠ€æœ€ç»ˆæ’å', fontsize=16, fontweight='bold')
        plt.xlabel('æ¨¡å‹åç§°', fontsize=12)
        plt.ylabel('æ€»å¾—åˆ†', fontsize=12)
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(score), ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/final_rankings.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_score_distribution(self, output_dir: str):
        """ç»˜åˆ¶å¾—åˆ†åˆ†å¸ƒå›¾"""
        model_scores = defaultdict(list)
        
        for result in self.detailed_results:
            for model_name, score in result['rankings']:
                model_scores[model_name].append(score)
        
        plt.figure(figsize=(12, 8))
        
        for i, (model_name, scores) in enumerate(model_scores.items()):
            plt.subplot(2, 2, i + 1)
            plt.hist(scores, bins=10, alpha=0.7, color=f'C{i}')
            plt.title(f'{model_name} å¾—åˆ†åˆ†å¸ƒ')
            plt.xlabel('å¾—åˆ†')
            plt.ylabel('é¢‘æ¬¡')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/score_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_topic_heatmap(self, output_dir: str):
        """ç»˜åˆ¶ä¸»é¢˜è¡¨ç°çƒ­åŠ›å›¾"""
        # æ„å»ºæ•°æ®çŸ©é˜µ
        models = [name for name, _ in self.final_rankings]
        topics = set()
        model_topic_scores = defaultdict(lambda: defaultdict(list))
        
        for result in self.detailed_results:
            topic = self._extract_topic(result['question'])
            topics.add(topic)
            for model_name, score in result['rankings']:
                model_topic_scores[model_name][topic].append(score)
        
        topics = sorted(list(topics))
        
        # è®¡ç®—å¹³å‡å¾—åˆ†çŸ©é˜µ
        score_matrix = []
        for model in models:
            model_row = []
            for topic in topics:
                scores = model_topic_scores[model][topic]
                avg_score = sum(scores) / len(scores) if scores else 0
                model_row.append(avg_score)
            score_matrix.append(model_row)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(score_matrix, 
                   xticklabels=topics, 
                   yticklabels=models,
                   annot=True, 
                   fmt='.1f', 
                   cmap='YlOrRd',
                   cbar_kws={'label': 'å¹³å‡å¾—åˆ†'})
        plt.title('å„æ¨¡å‹åœ¨ä¸åŒä¸»é¢˜çš„è¡¨ç°çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
        plt.xlabel('é—®é¢˜ä¸»é¢˜', fontsize=12)
        plt.ylabel('æ¨¡å‹åç§°', fontsize=12)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/topic_heatmap.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_judge_fairness(self, output_dir: str):
        """ç»˜åˆ¶è£åˆ¤å…¬æ­£æ€§åˆ†æå›¾"""
        judge_fairness = {}
        
        for result in self.detailed_results:
            judge = result['judge']
            rankings = result['rankings']
            scores = [score for _, score in rankings]
            
            # è®¡ç®—å¾—åˆ†æ–¹å·®ä½œä¸ºå…¬æ­£æ€§æŒ‡æ ‡ï¼ˆæ–¹å·®è¶Šå°è¶Šå…¬æ­£ï¼‰
            score_variance = pd.Series(scores).var() if len(scores) > 1 else 0
            
            if judge not in judge_fairness:
                judge_fairness[judge] = []
            judge_fairness[judge].append(score_variance)
        
        # è®¡ç®—å¹³å‡å…¬æ­£æ€§
        avg_fairness = {}
        for judge, variances in judge_fairness.items():
            avg_fairness[judge] = sum(variances) / len(variances)
        
        judges = list(avg_fairness.keys())
        fairness_scores = [10 - score for score in avg_fairness.values()]  # è½¬æ¢ä¸ºå…¬æ­£æ€§å¾—åˆ†
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(judges, fairness_scores, color='lightcoral')
        plt.title('å„æ¨¡å‹ä½œä¸ºè£åˆ¤çš„å…¬æ­£æ€§è¯„åˆ†', fontsize=14, fontweight='bold')
        plt.xlabel('è£åˆ¤æ¨¡å‹', fontsize=12)
        plt.ylabel('å…¬æ­£æ€§å¾—åˆ† (0-10)', fontsize=12)
        plt.ylim(0, 10)
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, fairness_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/judge_fairness.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def export_detailed_data(self, filename: str = "detailed_analysis.xlsx"):
        """å¯¼å‡ºè¯¦ç»†æ•°æ®åˆ°Excel"""
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # æœ€ç»ˆæ’å
            ranking_df = pd.DataFrame(self.final_rankings, columns=['æ¨¡å‹åç§°', 'æ€»å¾—åˆ†'])
            ranking_df.to_excel(writer, sheet_name='æœ€ç»ˆæ’å', index=False)
            
            # è¯¦ç»†ç»“æœ
            detailed_data = []
            for result in self.detailed_results:
                for model_name, score in result['rankings']:
                    detailed_data.append({
                        'é—®é¢˜ID': result['question_id'],
                        'é—®é¢˜å†…å®¹': result['question'][:100] + '...',
                        'è£åˆ¤æ¨¡å‹': result['judge'],
                        'å‚èµ›æ¨¡å‹': model_name,
                        'å¾—åˆ†': score,
                        'ä¸»é¢˜': self._extract_topic(result['question'])
                    })
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df.to_excel(writer, sheet_name='è¯¦ç»†ç»“æœ', index=False)
            
            # æ¨¡å‹è¡¨ç°ç»Ÿè®¡
            model_stats = self._analyze_model_performance()
            stats_data = []
            for model_name, stats in model_stats.items():
                stats_data.append({
                    'æ¨¡å‹åç§°': model_name,
                    'æ€»å¾—åˆ†': stats['total_score'],
                    'å¹³å‡å¾—åˆ†': stats['avg_score'],
                    'è£åˆ¤å…¬æ­£æ€§': stats['judge_fairness'],
                    'æ“…é•¿é¢†åŸŸ': ', '.join(stats['strong_topics'])
                })
            
            stats_df = pd.DataFrame(stats_data)
            stats_df.to_excel(writer, sheet_name='æ¨¡å‹ç»Ÿè®¡', index=False)

def analyze_competition_results(result_file: str = "competition_results.json"):
    """åˆ†æç«æŠ€ç»“æœçš„ä¸»å‡½æ•°"""
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            result_data = json.load(f)
        
        analyzer = ResultAnalyzer(result_data)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_summary_report()
        with open("competition_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ç”Ÿæˆå›¾è¡¨
        analyzer.generate_charts()
        
        # å¯¼å‡ºè¯¦ç»†æ•°æ®
        analyzer.export_detailed_data()
        
        print("åˆ†æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
        print("- competition_report.md: è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("- charts/: å›¾è¡¨æ–‡ä»¶å¤¹")
        print("- detailed_analysis.xlsx: è¯¦ç»†æ•°æ®è¡¨æ ¼")
        
        return analyzer
        
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶ {result_file}")
        return None
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{e}")
        return None

if __name__ == "__main__":
    analyze_competition_results()