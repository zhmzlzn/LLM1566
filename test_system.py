#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å¤§æ¨¡å‹ç«æŠ€ç³»ç»Ÿçš„å„ä¸ªç»„ä»¶åŠŸèƒ½
"""

import asyncio
import json
import os
import sys
from typing import Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from question_bank import QuestionBank, Question
from model_api import ModelAPIClient, ModelConfig
from result_analyzer import ResultAnalyzer

def test_question_bank():
    """æµ‹è¯•é—®é¢˜åº“åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•é—®é¢˜åº“åŠŸèƒ½...")
    
    try:
        bank = QuestionBank()
        stats = bank.get_statistics()
        
        print(f"âœ… é—®é¢˜åº“åŠ è½½æˆåŠŸ")
        print(f"   - æ€»é—®é¢˜æ•°: {stats['total_questions']}")
        print(f"   - ä¸»é¢˜æ•°: {len(stats['topics'])}")
        print(f"   - éš¾åº¦ç­‰çº§: {stats['available_difficulties']}")
        
        # æµ‹è¯•éšæœºè·å–é—®é¢˜
        random_questions = bank.get_random_questions(3, difficulty="medium")
        print(f"   - éšæœºè·å–3ä¸ªä¸­ç­‰éš¾åº¦é—®é¢˜: æˆåŠŸ")
        
        # æµ‹è¯•æŒ‰ä¸»é¢˜è·å–é—®é¢˜
        logic_questions = bank.get_questions_by_topic("é€»è¾‘æ¨ç†")
        print(f"   - é€»è¾‘æ¨ç†ç±»é—®é¢˜æ•°: {len(logic_questions)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é—®é¢˜åº“æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½...")
    
    try:
        # æµ‹è¯•ç¤ºä¾‹é…ç½®æ–‡ä»¶
        if os.path.exists("config.example.json"):
            with open("config.example.json", 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print(f"âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            print(f"   - é…ç½®çš„æ¨¡å‹æ•°: {len(config['models'])}")
            print(f"   - æœ€å°‘æ¨¡å‹æ•°è¦æ±‚: {config['competition_settings']['min_models']}")
            
            # éªŒè¯æ¨¡å‹é…ç½®æ ¼å¼
            for model in config['models']:
                required_fields = ['name', 'api_key', 'base_url', 'model', 'provider']
                if all(field in model for field in required_fields):
                    print(f"   - æ¨¡å‹ {model['name']} é…ç½®æ ¼å¼æ­£ç¡®")
                else:
                    print(f"   - âš ï¸ æ¨¡å‹ {model['name']} é…ç½®æ ¼å¼ä¸å®Œæ•´")
            
            return True
        else:
            print("âŒ ç¤ºä¾‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_api_client_structure():
    """æµ‹è¯•APIå®¢æˆ·ç«¯ç»“æ„"""
    print("\nğŸ§ª æµ‹è¯•APIå®¢æˆ·ç«¯ç»“æ„...")
    
    try:
        client = ModelAPIClient()
        
        # æµ‹è¯•æ”¯æŒçš„æä¾›å•†
        test_config = ModelConfig(
            name="Test Model",
            api_key="test_key",
            base_url="https://api.test.com",
            model="test-model",
            provider="test"
        )
        
        print(f"âœ… APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - æ”¯æŒçš„æ–¹æ³•: call_model")
        
        # æ£€æŸ¥å„æä¾›å•†çš„æ–¹æ³•æ˜¯å¦å­˜åœ¨
        providers = ['openai', 'anthropic', 'google', 'dashscope']
        for provider in providers:
            method_name = f"_call_{provider}"
            if hasattr(client, method_name):
                print(f"   - {provider} æä¾›å•†æ”¯æŒ: âœ…")
            else:
                print(f"   - {provider} æä¾›å•†æ”¯æŒ: âŒ")
        
        return True
        
    except Exception as e:
        print(f"âŒ APIå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_result_analyzer_structure():
    """æµ‹è¯•ç»“æœåˆ†æå™¨ç»“æ„"""
    print("\nğŸ§ª æµ‹è¯•ç»“æœåˆ†æå™¨ç»“æ„...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿç»“æœæ•°æ®
        mock_result = {
            "final_rankings": [
                ("GPT-4", 15),
                ("Claude-3", 12),
                ("Gemini-Pro", 10)
            ],
            "total_questions": 5,
            "total_rounds": 15,
            "detailed_results": [
                {
                    "question_id": 1,
                    "question": "æµ‹è¯•é—®é¢˜1",
                    "judge": "GPT-4",
                    "rankings": [("Claude-3", 9), ("Gemini-Pro", 7)],
                    "reasoning": "æµ‹è¯•è¯„åˆ¤ç†ç”±"
                }
            ]
        }
        
        analyzer = ResultAnalyzer(mock_result)
        
        print(f"âœ… ç»“æœåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - å‚èµ›æ¨¡å‹æ•°: {len(mock_result['final_rankings'])}")
        print(f"   - æ€»é—®é¢˜æ•°: {mock_result['total_questions']}")
        
        # æµ‹è¯•ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_summary_report()
        if report and len(report) > 100:
            print(f"   - æŠ¥å‘Šç”Ÿæˆ: âœ… (é•¿åº¦: {len(report)} å­—ç¬¦)")
        else:
            print(f"   - æŠ¥å‘Šç”Ÿæˆ: âŒ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç»“æœåˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_file_structure():
    """æµ‹è¯•æ–‡ä»¶ç»“æ„å®Œæ•´æ€§"""
    print("\nğŸ§ª æµ‹è¯•æ–‡ä»¶ç»“æ„å®Œæ•´æ€§...")
    
    required_files = [
        "llm_competition.py",
        "model_api.py",
        "question_bank.py",
        "result_analyzer.py",
        "cli.py",
        "config.json",
        "config.example.json",
        "requirements.txt",
        "README.md"
    ]
    
    missing_files = []
    existing_files = []
    
    for file in required_files:
        if os.path.exists(file):
            existing_files.append(file)
            print(f"   - {file}: âœ…")
        else:
            missing_files.append(file)
            print(f"   - {file}: âŒ")
    
    if not missing_files:
        print(f"âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨ ({len(existing_files)}/{len(required_files)})")
        return True
    else:
        print(f"âš ï¸ ç¼ºå°‘ {len(missing_files)} ä¸ªæ–‡ä»¶: {missing_files}")
        return False

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    modules_to_test = [
        ("llm_competition", "LLMCompetition"),
        ("model_api", "ModelAPIClient"),
        ("question_bank", "QuestionBank"),
        ("result_analyzer", "ResultAnalyzer")
    ]
    
    success_count = 0
    
    for module_name, class_name in modules_to_test:
        try:
            module = __import__(module_name)
            if hasattr(module, class_name):
                print(f"   - {module_name}.{class_name}: âœ…")
                success_count += 1
            else:
                print(f"   - {module_name}.{class_name}: âŒ (ç±»ä¸å­˜åœ¨)")
        except ImportError as e:
            print(f"   - {module_name}: âŒ (å¯¼å…¥å¤±è´¥: {e})")
        except Exception as e:
            print(f"   - {module_name}: âŒ (é”™è¯¯: {e})")
    
    if success_count == len(modules_to_test):
        print(f"âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ ({success_count}/{len(modules_to_test)})")
        return True
    else:
        print(f"âš ï¸ {len(modules_to_test) - success_count} ä¸ªæ¨¡å—å¯¼å…¥å¤±è´¥")
        return False

async def test_mock_competition():
    """æµ‹è¯•æ¨¡æ‹Ÿç«æŠ€æµç¨‹"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡æ‹Ÿç«æŠ€æµç¨‹...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        mock_config = {
            "models": [
                {
                    "name": "Mock-GPT",
                    "api_key": "mock_key_1",
                    "base_url": "https://mock.api.com",
                    "model": "mock-gpt",
                    "provider": "mock"
                },
                {
                    "name": "Mock-Claude",
                    "api_key": "mock_key_2",
                    "base_url": "https://mock.api.com",
                    "model": "mock-claude",
                    "provider": "mock"
                },
                {
                    "name": "Mock-Gemini",
                    "api_key": "mock_key_3",
                    "base_url": "https://mock.api.com",
                    "model": "mock-gemini",
                    "provider": "mock"
                }
            ],
            "competition_settings": {
                "min_models": 3,
                "question_generation": {
                    "enabled": False,
                    "count": 2,
                    "difficulty": "easy"
                },
                "scoring": {
                    "first_place": 3,
                    "second_place": 2,
                    "third_place": 1,
                    "other_place": 0
                }
            }
        }
        
        # ä¿å­˜æ¨¡æ‹Ÿé…ç½®
        with open("test_config.json", 'w', encoding='utf-8') as f:
            json.dump(mock_config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ¨¡æ‹Ÿé…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   - æ¨¡æ‹Ÿæ¨¡å‹æ•°: {len(mock_config['models'])}")
        print(f"   - æœ€å°‘æ¨¡å‹è¦æ±‚: {mock_config['competition_settings']['min_models']}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists("test_config.json"):
            os.remove("test_config.json")
            print(f"   - æµ‹è¯•æ–‡ä»¶æ¸…ç†: âœ…")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡æ‹Ÿç«æŠ€æµ‹è¯•å¤±è´¥: {e}")
        return False

def generate_test_report(results: Dict[str, bool]):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    failed_tests = total_tests - passed_tests
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   - æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"   - é€šè¿‡æµ‹è¯•: {passed_tests} âœ…")
    print(f"   - å¤±è´¥æµ‹è¯•: {failed_tests} âŒ")
    print(f"   - æˆåŠŸç‡: {(passed_tests/total_tests)*100:.1f}%")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœ:")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   - {test_name}: {status}")
    
    if failed_tests == 0:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå‡†å¤‡å°±ç»ªã€‚")
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"   1. å¤åˆ¶ config.example.json ä¸º config.json")
        print(f"   2. å¡«å…¥çœŸå®çš„APIå¯†é’¥")
        print(f"   3. è¿è¡Œ python cli.py run å¼€å§‹ç«æŠ€")
    else:
        print(f"\nâš ï¸ å‘ç° {failed_tests} ä¸ªé—®é¢˜ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")
        print(f"\nğŸ”§ å»ºè®®æ“ä½œ:")
        print(f"   1. æ£€æŸ¥ç¼ºå¤±çš„æ–‡ä»¶")
        print(f"   2. å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…")
        print(f"   3. ä¿®å¤å¯¼å…¥é”™è¯¯")
    
    print("\n" + "="*60)

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¤§æ¨¡å‹ç«æŠ€ç³»ç»Ÿ - ç³»ç»Ÿæµ‹è¯•")
    print("="*60)
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results = {}
    
    test_results["æ–‡ä»¶ç»“æ„å®Œæ•´æ€§"] = test_file_structure()
    test_results["æ¨¡å—å¯¼å…¥"] = test_imports()
    test_results["é…ç½®æ–‡ä»¶åŠ è½½"] = test_config_loading()
    test_results["é—®é¢˜åº“åŠŸèƒ½"] = test_question_bank()
    test_results["APIå®¢æˆ·ç«¯ç»“æ„"] = test_api_client_structure()
    test_results["ç»“æœåˆ†æå™¨ç»“æ„"] = test_result_analyzer_structure()
    test_results["æ¨¡æ‹Ÿç«æŠ€æµç¨‹"] = await test_mock_competition()
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_test_report(test_results)

if __name__ == "__main__":
    asyncio.run(main())